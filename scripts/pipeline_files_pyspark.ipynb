{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf3e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version : 4.0.1\n",
      "Connecteur Delta chargé avec succès.\n"
     ]
    }
   ],
   "source": [
    "# This notebook is the 'static' version of the pipeline, it loads data \n",
    "# from JSON files in batch mode and writes them into a Delta table.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, TimestampType\n",
    "from pathlib import Path\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_package = \"io.delta:delta-spark_2.13:4.0.0\"\n",
    "\n",
    "builder = (SparkSession.builder\n",
    "    .appName(\"SmartTech_Streaming\")\n",
    "    .config(\"spark.jars.packages\", delta_package)\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(f\"Spark version : {spark.version}\")\n",
    "print(\"Connecteur Delta chargé avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaadfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the streaming dataframe schema, with a 'corrupt_reccord' column in case of errors\n",
    "schema_sensor = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"building\", StringType(), True),\n",
    "    StructField(\"floor\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"value\", FloatType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6de009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the differents paths with Path objects\n",
    "INPUT_PATH = Path.cwd().parent/\"data\"/\"sensor_data\"\n",
    "BRONZE_PATH = Path.cwd().parent/\"data\"/\"out\"/\"delta_bronze\"\n",
    "CHECKPOINT_PATH = Path.cwd().parent/\"data\"/\"out\"/\"checkpoint\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the streaming dataframe - lazy execution\n",
    "df_stream = (spark.readStream\n",
    "             .format(\"json\")\n",
    "             .schema(schema_sensor)\n",
    "             .option(\"maxFilesPerTrigger\", 1)\n",
    "             .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "             .load(str(INPUT_PATH))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30950f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we filter the Null values - lazy execution\n",
    "df_stream_clean = df_stream.filter(col(\"timestamp\")\n",
    "                                   .isNotNull() & \n",
    "                                   col(\"device_id\").isNotNull() & \n",
    "                                   col(\"building\").isNotNull() & \n",
    "                                   col(\"floor\").isNotNull() & \n",
    "                                   col(\"type\").isNotNull() & \n",
    "                                   col(\"value\").isNotNull() & \n",
    "                                   col(\"unit\").isNotNull())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a614b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/16 11:18:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/16 11:18:16 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/12/16 11:18:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 4367 milliseconds\n",
      "25/12/16 11:18:21 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1089 milliseconds\n",
      "25/12/16 11:18:22 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1046 milliseconds\n",
      "25/12/16 11:18:29 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1537 milliseconds\n",
      "25/12/16 11:18:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1457 milliseconds\n",
      "25/12/16 11:18:32 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1007 milliseconds\n",
      "25/12/16 11:18:39 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1279 milliseconds\n",
      "25/12/16 11:18:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1170 milliseconds\n",
      "25/12/16 11:18:45 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1065 milliseconds\n",
      "25/12/16 11:18:49 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1295 milliseconds\n",
      "25/12/16 11:18:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1107 milliseconds\n",
      "25/12/16 11:18:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1433 milliseconds\n",
      "25/12/16 11:19:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1258 milliseconds\n",
      "25/12/16 11:19:09 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1391 milliseconds\n",
      "25/12/16 11:19:19 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1579 milliseconds\n",
      "25/12/16 11:19:29 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1396 milliseconds\n",
      "25/12/16 11:19:39 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1322 milliseconds\n",
      "25/12/16 11:19:49 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1562 milliseconds\n",
      "25/12/16 11:19:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000} milliseconds, but spent 1558 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# writing the clean streaming dataframe into a Delta table\n",
    "query = (df_stream_clean.writeStream          \n",
    "    .format(\"delta\")                         \n",
    "    .outputMode(\"append\")                    \n",
    "    .option(\"checkpointLocation\", str(CHECKPOINT_PATH))\n",
    "    .option(\"path\", str(BRONZE_PATH))\n",
    "    .trigger(processingTime='1 seconds')    \n",
    "    .toTable(\"bronze_sensor_data\")              \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_delta = DeltaTable.forPath(spark, str(BRONZE_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c02d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+----------------+--------------------------------------------------------------------------------------+\n",
      "|version|timestamp              |operation       |operationMetrics                                                                      |\n",
      "+-------+-----------------------+----------------+--------------------------------------------------------------------------------------+\n",
      "|100    |2025-12-16 11:19:58.702|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2591}|\n",
      "|99     |2025-12-16 11:19:57.854|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2550}|\n",
      "|98     |2025-12-16 11:19:56.796|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2590}|\n",
      "|97     |2025-12-16 11:19:55.678|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2548}|\n",
      "|96     |2025-12-16 11:19:54.648|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2576}|\n",
      "|95     |2025-12-16 11:19:53.599|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2554}|\n",
      "|94     |2025-12-16 11:19:52.67 |STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2548}|\n",
      "|93     |2025-12-16 11:19:51.712|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2521}|\n",
      "|92     |2025-12-16 11:19:50.963|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2566}|\n",
      "|91     |2025-12-16 11:19:50.195|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2568}|\n",
      "|90     |2025-12-16 11:19:48.787|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2562}|\n",
      "|89     |2025-12-16 11:19:47.618|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2548}|\n",
      "|88     |2025-12-16 11:19:46.675|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2574}|\n",
      "|87     |2025-12-16 11:19:45.605|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2550}|\n",
      "|86     |2025-12-16 11:19:44.68 |STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2518}|\n",
      "|85     |2025-12-16 11:19:43.751|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2537}|\n",
      "|84     |2025-12-16 11:19:42.609|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2554}|\n",
      "|83     |2025-12-16 11:19:41.605|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2537}|\n",
      "|82     |2025-12-16 11:19:40.675|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2518}|\n",
      "|81     |2025-12-16 11:19:39.951|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2576}|\n",
      "|80     |2025-12-16 11:19:38.628|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2565}|\n",
      "|79     |2025-12-16 11:19:37.654|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2550}|\n",
      "|78     |2025-12-16 11:19:36.631|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2542}|\n",
      "|77     |2025-12-16 11:19:35.753|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2542}|\n",
      "|76     |2025-12-16 11:19:34.653|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2550}|\n",
      "|75     |2025-12-16 11:19:33.599|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2526}|\n",
      "|74     |2025-12-16 11:19:32.69 |STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2552}|\n",
      "|73     |2025-12-16 11:19:31.587|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2597}|\n",
      "|72     |2025-12-16 11:19:30.767|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2536}|\n",
      "|71     |2025-12-16 11:19:30.032|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2549}|\n",
      "|70     |2025-12-16 11:19:28.608|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2537}|\n",
      "|69     |2025-12-16 11:19:27.622|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2603}|\n",
      "|68     |2025-12-16 11:19:26.744|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2548}|\n",
      "|67     |2025-12-16 11:19:25.627|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2548}|\n",
      "|66     |2025-12-16 11:19:24.603|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2543}|\n",
      "|65     |2025-12-16 11:19:23.617|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2569}|\n",
      "|64     |2025-12-16 11:19:22.611|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2592}|\n",
      "|63     |2025-12-16 11:19:21.614|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2547}|\n",
      "|62     |2025-12-16 11:19:20.942|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2523}|\n",
      "|61     |2025-12-16 11:19:20.218|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2602}|\n",
      "|60     |2025-12-16 11:19:18.798|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2536}|\n",
      "|59     |2025-12-16 11:19:17.588|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2556}|\n",
      "|58     |2025-12-16 11:19:16.584|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2562}|\n",
      "|57     |2025-12-16 11:19:15.605|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2575}|\n",
      "|56     |2025-12-16 11:19:14.644|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2584}|\n",
      "|55     |2025-12-16 11:19:13.748|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2574}|\n",
      "|54     |2025-12-16 11:19:12.542|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2590}|\n",
      "|53     |2025-12-16 11:19:11.576|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2548}|\n",
      "|52     |2025-12-16 11:19:10.811|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2580}|\n",
      "|51     |2025-12-16 11:19:10.125|STREAMING UPDATE|{numRemovedFiles -> 0, numAddedFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2538}|\n",
      "+-------+-----------------------+----------------+--------------------------------------------------------------------------------------+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "source": [
    "delta_table = DeltaTable.forPath(spark, str(BRONZE_PATH))\n",
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "961a1e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+--------+-----+------------------+------+----+---------------+\n",
      "|          timestamp|        device_id|building|floor|              type| value|unit|_corrupt_record|\n",
      "+-------------------+-----------------+--------+-----+------------------+------+----+---------------+\n",
      "|2025-01-12 09:37:39|  sensor-temp-003|       A|    1|       temperature|  27.6|  °C|           NULL|\n",
      "|2025-01-12 09:37:43|   sensor-co2-020|       A|    3|               co2| 758.0| ppm|           NULL|\n",
      "|2025-01-12 09:37:47|sensor-energy-010|       B|    3|energy_consumption| 171.6| kWh|           NULL|\n",
      "|2025-01-12 09:37:50|  sensor-temp-001|       B|    2|       temperature|  27.7|  °C|           NULL|\n",
      "|2025-01-12 09:37:53|   sensor-hum-002|       A|    1|          humidity|  30.9|   %|           NULL|\n",
      "|2025-01-12 09:37:24|   sensor-co2-020|       A|    1|               co2| 949.0| ppm|           NULL|\n",
      "|2025-01-12 09:37:27|   sensor-hum-002|       A|    1|          humidity|  53.6|   %|           NULL|\n",
      "|2025-01-12 09:37:29|   sensor-co2-020|       A|    3|               co2| 888.0| ppm|           NULL|\n",
      "|2025-01-12 09:37:34|   sensor-co2-020|       B|    2|               co2| 941.0| ppm|           NULL|\n",
      "|2025-01-12 09:37:37|  sensor-temp-001|       A|    1|       temperature|  24.2|  °C|           NULL|\n",
      "|2025-01-12 09:34:35|   sensor-co2-020|       B|    3|               co2| 817.0| ppm|           NULL|\n",
      "|2025-01-12 09:34:36|  sensor-temp-003|       B|    3|       temperature|  23.2|  °C|           NULL|\n",
      "|2025-01-12 09:34:40|  sensor-temp-003|       B|    1|       temperature|  19.9|  °C|           NULL|\n",
      "|2025-01-12 09:34:42|   sensor-co2-020|       B|    2|               co2|1051.0| ppm|           NULL|\n",
      "|2025-01-12 09:34:44|   sensor-hum-002|       B|    2|          humidity|  39.5|   %|           NULL|\n",
      "|2025-01-12 09:42:44|   sensor-co2-020|       B|    3|               co2| 571.0| ppm|           NULL|\n",
      "|2025-01-12 09:42:48|   sensor-hum-002|       A|    3|          humidity|  42.7|   %|           NULL|\n",
      "|2025-01-12 09:42:50|   sensor-co2-020|       A|    3|               co2| 922.0| ppm|           NULL|\n",
      "|2025-01-12 09:42:55|  sensor-temp-003|       B|    2|       temperature|  21.9|  °C|           NULL|\n",
      "|2025-01-12 09:43:00|   sensor-hum-002|       A|    3|          humidity|  33.5|   %|           NULL|\n",
      "+-------------------+-----------------+--------+-----+------------------+------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT *  FROM bronze_sensor_data LIMIT 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ec4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esther_spark_streaming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
